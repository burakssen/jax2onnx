# jax2onnx



![img.png](https://enpasos.github.io/jax2onnx/images/img1.png)

`jax2onnx` converts your JAX/Flax model directly into the ONNX format.  

### **Approach**
Components can be easily added as plugins, including their test cases, which are automatically picked up by **pytest**. Each test case sends random input tensors through the JAX/Flax model and compares the output with the ONNX model to ensure correctness.

This library follows a **test-driven and demand-driven** approach, giving you **full control** over how JAX/Flax components are mapped to ONNX‚Äî**no hidden magic, no black-box abstraction**. While it may not cover every use case out of the box, you can **extend it by adding your own plugins** and contribute them back to the project. üöÄ

### **Supported JAX/ONNX Components**

[test](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/multihead_attention_model.onnx)

<!-- AUTOGENERATED TABLE START -->

| JAX Component | ONNX Components | Since | Testcases |
|:-------------|:---------------|:------|:---------|
| [flax.nnx.BatchNorm](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.BatchNorm) | [BatchNormalization](https://onnx.ai/onnx/operators/onnx__BatchNormalization.html) | v0.1.0 | batchnorm ‚úÖ |
| [flax.nnx.Conv](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Conv) | [Conv](https://onnx.ai/onnx/operators/onnx__Conv.html) | v0.1.0 | conv_3x3 ‚úÖ<br>conv_5x5_stride2 ‚ùå |
| [flax.nnx.Dropout](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/stochastic.html#flax.nnx.Dropout) | [Dropout](https://onnx.ai/onnx/operators/onnx__Dropout.html) | v0.1.0 | dropout ‚úÖ<br>dropout_1d ‚úÖ<br>dropout_2d ‚úÖ<br>dropout_3d ‚úÖ<br>dropout_4d ‚úÖ<br>dropout_high ‚úÖ<br>dropout_low ‚úÖ |
| [flax.nnx.LayerNorm](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.LayerNorm) | [LayerNormalization](https://onnx.ai/onnx/operators/onnx__LayerNormalization.html) | v0.1.0 | layernorm_default ‚úÖ<br>layernorm_multiaxis ‚úÖ |
| [flax.nnx.Linear](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Linear) | [Gemm](https://onnx.ai/onnx/operators/onnx__Gemm.html) | v0.1.0 | linear ‚úÖ<br>linear_2 ‚úÖ |
| [flax.nnx.LinearGeneral](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.LinearGeneral) | [Reshape](https://onnx.ai/onnx/operators/onnx__Reshape.html)<br>[MatMul](https://onnx.ai/onnx/operators/onnx__MatMul.html) | v0.1.0 | linear_general ‚úÖ<br>linear_general_2 ‚úÖ<br>linear_general_mha_projection ‚úÖ |
| [flax.nnx.MultiHeadAttention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.MultiHeadAttention) | [Reshape](https://onnx.ai/onnx/operators/onnx__Reshape.html)<br>[MatMul](https://onnx.ai/onnx/operators/onnx__MatMul.html)<br>[Constant](https://onnx.ai/onnx/operators/onnx__Constant.html)<br>[Einsum](https://onnx.ai/onnx/operators/onnx__Einsum.html)<br>[Mul](https://onnx.ai/onnx/operators/onnx__Mul.html)<br>[Softmax](https://onnx.ai/onnx/operators/onnx__Softmax.html) | v0.1.0 | multihead_attention ‚úÖ |
| [flax.nnx.avg_pool](https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.avg_pool) | [AveragePool](https://onnx.ai/onnx/operators/onnx__AveragePool.html) | v0.1.0 | avg_pool ‚úÖ |
| [flax.nnx.dot_product_attention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.dot_product_attention) | [Constant](https://onnx.ai/onnx/operators/onnx__Constant.html)<br>[Einsum](https://onnx.ai/onnx/operators/onnx__Einsum.html)<br>[Mul](https://onnx.ai/onnx/operators/onnx__Mul.html)<br>[Softmax](https://onnx.ai/onnx/operators/onnx__Softmax.html) | v0.1.0 | dot_product_attention ‚úÖ<br>dot_product_attention_shape_check ‚úÖ<br>dot_product_attention_softmax_axis ‚úÖ |
| [flax.nnx.elu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.elu) | [Elu](https://onnx.ai/onnx/operators/onnx__Elu.html) | v0.1.0 | elu ‚úÖ |
| [flax.nnx.gelu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.gelu) | [Gelu](https://onnx.ai/onnx/operators/onnx__Gelu.html) | v0.1.0 | gelu ‚úÖ<br>gelu2 ‚úÖ<br>gelu3 ‚úÖ |
| [flax.nnx.leaky_relu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.leaky_relu) | [LeakyRelu](https://onnx.ai/onnx/operators/onnx__LeakyRelu.html) | v0.1.0 | leaky_relu ‚úÖ |
| [flax.nnx.log_softmax](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.log_softmax) | [LogSoftmax](https://onnx.ai/onnx/operators/onnx__LogSoftmax.html) | v0.1.0 | log_softmax ‚úÖ |
| [flax.nnx.max_pool](https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.max_pool) | [MaxPool](https://onnx.ai/onnx/operators/onnx__MaxPool.html) | v0.1.0 | max_pool ‚úÖ |
| [flax.nnx.relu](https://docs.jax.dev/en/latest/_autosummary/jax.nn.relu.html#jax.nn.relu) | [Relu](https://onnx.ai/onnx/operators/onnx__Relu.html) | v0.1.0 | relu ‚úÖ |
| [flax.nnx.sigmoid](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.sigmoid) | [Sigmoid](https://onnx.ai/onnx/operators/onnx__Sigmoid.html) | v0.1.0 | sigmoid ‚úÖ |
| [flax.nnx.softmax](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.softmax) | [Softmax](https://onnx.ai/onnx/operators/onnx__Softmax.html) | v0.1.0 | softmax ‚úÖ |
| [flax.nnx.softplus](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.softplus) | [Softplus](https://onnx.ai/onnx/operators/onnx__Softplus.html) | v0.1.0 | softplus ‚úÖ |
| [flax.nnx.tanh](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.tanh) | [Tanh](https://onnx.ai/onnx/operators/onnx__Tanh.html) | v0.1.0 | tanh ‚úÖ |
| [jax.lax.slice](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.slice.html) | [Slice](https://onnx.ai/onnx/operators/onnx__Slice.html) | v0.1.0 | slice_basic ‚úÖ<br>slice_last_column ‚úÖ<br>slice_out_of_bounds ‚úÖ<br>slice_single_element ‚úÖ<br>slice_with_stride ‚úÖ |
| [jax.numpy.add](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.add.html) | [Add](https://onnx.ai/onnx/operators/onnx__Add.html) | v0.1.0 | add ‚úÖ |
| [jax.numpy.concat](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.concat.html) | [Concat](https://onnx.ai/onnx/operators/onnx__Concat.html) | v0.1.0 | concat ‚úÖ |
| [jax.numpy.einsum](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html) | [Einsum](https://onnx.ai/onnx/operators/onnx__Einsum.html) | v0.1.0 | einsum_attention ‚úÖ<br>einsum_batch_matmul ‚úÖ<br>einsum_matmul ‚úÖ |
| [jax.numpy.matmul](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html) | [MatMul](https://onnx.ai/onnx/operators/onnx__MatMul.html) | v0.1.0 | matmul_2d ‚úÖ<br>matmul_3d ‚úÖ<br>matmul_4d ‚úÖ |
| [jax.numpy.reshape](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.reshape.html) | [Reshape](https://onnx.ai/onnx/operators/onnx__Reshape.html) | v0.1.0 | reshape ‚úÖ<br>reshape_batch ‚úÖ<br>reshape_dynamic ‚úÖ |
| [jax.numpy.squeeze](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.squeeze.html) | [Squeeze](https://onnx.ai/onnx/operators/onnx__Squeeze.html) | v0.1.0 | squeeze_multiple_dims ‚úÖ<br>squeeze_no_change ‚úÖ<br>squeeze_single_dim ‚úÖ<br>squeeze_vit_output ‚úÖ |
| [jax.numpy.tile](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html) | [Tile](https://onnx.ai/onnx/operators/onnx__Tile.html) | v0.1.0 | tile_1d ‚úÖ<br>tile_2x ‚úÖ<br>tile_batch_dim ‚úÖ<br>tile_large ‚úÖ |
| [jax.numpy.transpose](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html) | [Transpose](https://onnx.ai/onnx/operators/onnx__Transpose.html) | v0.1.0 | transpose_4d ‚úÖ<br>transpose_basic ‚úÖ<br>transpose_high_dim ‚úÖ<br>transpose_reverse ‚úÖ<br>transpose_square_matrix ‚úÖ |

<!-- AUTOGENERATED TABLE END -->

‚úÖ = passed<br>
‚ùå = failed<br>
‚ûñ = no tests, yet

Examples

<!-- AUTOGENERATED EXAMPLES TABLE START -->

| Component | Description | Children | Since | Testcases |
|:----------|:------------|:---------|:------|:---------|
| CNN | A MNIST CNN model with convolutional and linear layers. | flax.nnx.Conv<br>flax.nnx.Linear<br>flax.nnx.relu<br>flax.nnx.avg_pool<br>flax.nnx.reshape<br>flax.nnx.log_softmax | v0.1.0 | mnist_cnn ‚úÖ |
| CNN | A MNIST CNN model with convolutional, layer norm, pooling, dropout, and linear layers. | flax.nnx.Conv<br>flax.nnx.Linear<br>flax.nnx.relu<br>flax.nnx.avg_pool<br>flax.nnx.reshape<br>flax.nnx.log_softmax<br>flax.nnx.LayerNorm<br>flax.nnx.Dropout<br>flax.nnx.max_pool | v0.1.0 | mnist_cnn_2 ‚úÖ |
| MLP | A simple Multi-Layer Perceptron (MLP) with BatchNorm, Dropout, and GELU activation. | flax.nnx.Linear<br>flax.nnx.BatchNorm<br>flax.nnx.Dropout<br>flax.nnx.gelu<br>flax.nnx.Linear | v0.1.0 | mlp ‚úÖ |
| MLP Block | MLP in Transformer | flax.nnx.Linear<br>flax.nnx.Dropout<br>flax.nnx.gelu | v0.1.0 | mlp_block ‚úÖ |
| PatchEmbedding | Cutting the image into patches and linearly embedding them. | flax.nnx.Linear<br>jax.numpy.Transpose<br>jax.numpy.Reshape | v0.1.0 | patch_embedding ‚úÖ |
| TransformerBlock | Transformer from 'Attention Is All You Need.' | flax.nnx.MultiHeadAttention<br>flax.nnx.LayerNorm<br>flax.nnx.Dropout<br>MLPBlock<br>flax.nnx.Dropout | v0.1.0 | transformer_block ‚úÖ |
| ViT | A MNIST Vision Tansformer (ViT) model | PatchEmbedding<br>TransformerBlock<br>flax.nnx.Linear<br>flax.nnx.LayerNorm | v0.1.0 | mnist_vit ‚úÖ |

<!-- AUTOGENERATED EXAMPLES TABLE END -->


Versions of Major Dependencies

| Library       | jax2onnx v0.1.0 | 
|:--------------|:----------------| 
| `JAX`         | v0.5.0          | 
| `Flax`        | v0.10.3         | 
| `onnx`        | v1.17.0         |  
| `onnxruntime` | v1.20.1         |  

Note: for more details look into the `pyproject.toml` file



### **Usage**
Import the `jax2onnx` module, implement the `to_onnx` function to your Module class and use the `jax2onnx.to_onnx.to_onnx`
function to convert your model to ONNX format. See at the examples provided in the `examples` directory.


Example of an MLP with the `to_onnx` function implemented:

```py
class MLP(nnx.Module):
    def __init__(self, din: int, dmid: int, dout: int, *, rngs=nnx.Rngs(0)): 
        self.linear1 = nnx.Linear(din, dmid, rngs=rngs)
        self.batch_norm = nnx.BatchNorm(dmid, rngs=rngs)
        self.dropout = nnx.Dropout(rate=0.1, rngs=rngs)
        self.activation = jax.nn.gelu
        self.linear2 = nnx.Linear(dmid, dout, rngs=rngs)

    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -> jnp.ndarray: 
        x = self.linear1(x)
        x = self.batch_norm(x)
        x = self.dropout(x, deterministic=deterministic)
        x = self.activation(x)
        return self.linear2(x)

    def to_onnx(self, z, parameters=None): 
        for layer in [self.linear1, self.batch_norm, self.dropout, self.activation, self.linear2]:
            z = layer.to_onnx(z, parameters)
        return z
```

To export the model to ONNX format, use the `to_onnx` function:
```py
from jax2onnx.to_onnx import to_onnx
to_onnx(
    file_name="mlp.onnx",
    jax_model=MLP(din=30, dmid=20, dout=10, rngs=nnx.Rngs(17)),
    input_shapes=[(1, 30)]
)
```

### **How to Contribute**

If you'd like to see a model or function supported, consider contributing by adding a plugin for an existing   
module or function under the `jax2onnx/plugins` directory. Or you can add an example to the `examples` directory.
Certainly any other improvements are welcome as well.

### **Installation**

To install `jax2onnx`:

```bash
pip install -i https://test.pypi.org/simple/ jax2onnx
```

t.b.d.:
```bash
pip install jax2onnx  
```

### **License**
This project is licensed under the terms of the Apache License, Version 2.0. See the `LICENSE` file for details.

