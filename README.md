# jax2onnx


![img.png](https://enpasos.github.io/jax2onnx/images/img1.png)

`jax2onnx` converts your JAX/Flax model directly into the ONNX format.  

### **Approach**
Components can be easily added as plugins, including their test cases, which are automatically picked up by **pytest**. Each test case sends random input tensors through the JAX/Flax model and compares the output with the ONNX model to ensure correctness.

This library follows a **test-driven and demand-driven** approach, giving you **full control** over how JAX/Flax components are mapped to ONNX—**no hidden magic, no black-box abstraction**. While it may not cover every use case out of the box, you can **extend it by adding your own plugins** and contribute them back to the project. 🚀

### **Supported JAX/ONNX Components**


 
<!-- AUTOGENERATED TABLE START -->

| JAX Component | ONNX Components | Since | Testcases |
|:-------------|:---------------|:------|:---------|
| [flax.nnx.BatchNorm](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.BatchNorm) | [BatchNormalization](https://onnx.ai/onnx/operators/onnx__BatchNormalization.html) | v0.1.0 | [`batchnorm`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/batchnorm_model.onnx) ✅ |
| [flax.nnx.Conv](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Conv) | [Conv](https://onnx.ai/onnx/operators/onnx__Conv.html) | v0.1.0 | [`conv_3x3_1`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/conv_3x3_1_model.onnx) ✅<br>[`conv_3x3_2`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/conv_3x3_2_model.onnx) ✅<br>[`conv_3x3_3`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/conv_3x3_3_model.onnx) ✅ |
| [flax.nnx.Dropout](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/stochastic.html#flax.nnx.Dropout) | [Dropout](https://onnx.ai/onnx/operators/onnx__Dropout.html) | v0.1.0 | [`dropout`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dropout_model.onnx) ✅<br>[`dropout_1d`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dropout_1d_model.onnx) ✅<br>[`dropout_2d`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dropout_2d_model.onnx) ✅<br>[`dropout_3d`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dropout_3d_model.onnx) ✅<br>[`dropout_4d`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dropout_4d_model.onnx) ✅<br>[`dropout_high`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dropout_high_model.onnx) ✅<br>[`dropout_low`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dropout_low_model.onnx) ✅ |
| [flax.nnx.LayerNorm](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.LayerNorm) | [LayerNormalization](https://onnx.ai/onnx/operators/onnx__LayerNormalization.html) | v0.1.0 | [`layernorm_default`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/layernorm_default_model.onnx) ✅<br>[`layernorm_multiaxis`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/layernorm_multiaxis_model.onnx) ✅ |
| [flax.nnx.Linear](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Linear) | [Gemm](https://onnx.ai/onnx/operators/onnx__Gemm.html) | v0.1.0 | [`linear`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/linear_model.onnx) ✅<br>[`linear_2`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/linear_2_model.onnx) ✅ |
| [flax.nnx.LinearGeneral](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.LinearGeneral) | [Reshape](https://onnx.ai/onnx/operators/onnx__Reshape.html)<br>[MatMul](https://onnx.ai/onnx/operators/onnx__MatMul.html) | v0.1.0 | [`linear_general`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/linear_general_model.onnx) ✅<br>[`linear_general_2`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/linear_general_2_model.onnx) ✅<br>[`linear_general_mha_projection`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/linear_general_mha_projection_model.onnx) ✅ |
| [flax.nnx.MultiHeadAttention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.MultiHeadAttention) | [Reshape](https://onnx.ai/onnx/operators/onnx__Reshape.html)<br>[MatMul](https://onnx.ai/onnx/operators/onnx__MatMul.html)<br>[Constant](https://onnx.ai/onnx/operators/onnx__Constant.html)<br>[Einsum](https://onnx.ai/onnx/operators/onnx__Einsum.html)<br>[Mul](https://onnx.ai/onnx/operators/onnx__Mul.html)<br>[Softmax](https://onnx.ai/onnx/operators/onnx__Softmax.html) | v0.1.0 | [`multihead_attention`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/multihead_attention_model.onnx) ✅ |
| [flax.nnx.avg_pool](https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.avg_pool) | [AveragePool](https://onnx.ai/onnx/operators/onnx__AveragePool.html) | v0.1.0 | [`avg_pool`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/avg_pool_model.onnx) ✅ |
| [flax.nnx.dot_product_attention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.dot_product_attention) | [Constant](https://onnx.ai/onnx/operators/onnx__Constant.html)<br>[Einsum](https://onnx.ai/onnx/operators/onnx__Einsum.html)<br>[Mul](https://onnx.ai/onnx/operators/onnx__Mul.html)<br>[Softmax](https://onnx.ai/onnx/operators/onnx__Softmax.html) | v0.1.0 | [`dot_product_attention`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dot_product_attention_model.onnx) ✅<br>[`dot_product_attention_shape_check`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dot_product_attention_shape_check_model.onnx) ✅<br>[`dot_product_attention_softmax_axis`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/dot_product_attention_softmax_axis_model.onnx) ✅ |
| [flax.nnx.elu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.elu) | [Elu](https://onnx.ai/onnx/operators/onnx__Elu.html) | v0.1.0 | [`elu`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/elu_model.onnx) ✅ |
| [flax.nnx.gelu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.gelu) | [Gelu](https://onnx.ai/onnx/operators/onnx__Gelu.html) | v0.1.0 | [`gelu`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/gelu_model.onnx) ✅<br>[`gelu2`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/gelu2_model.onnx) ✅<br>[`gelu3`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/gelu3_model.onnx) ✅ |
| [flax.nnx.leaky_relu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.leaky_relu) | [LeakyRelu](https://onnx.ai/onnx/operators/onnx__LeakyRelu.html) | v0.1.0 | [`leaky_relu`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/leaky_relu_model.onnx) ✅ |
| [flax.nnx.log_softmax](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.log_softmax) | [LogSoftmax](https://onnx.ai/onnx/operators/onnx__LogSoftmax.html) | v0.1.0 | [`log_softmax`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/log_softmax_model.onnx) ✅ |
| [flax.nnx.max_pool](https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.max_pool) | [MaxPool](https://onnx.ai/onnx/operators/onnx__MaxPool.html) | v0.1.0 | [`max_pool`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/max_pool_model.onnx) ✅ |
| [flax.nnx.relu](https://docs.jax.dev/en/latest/_autosummary/jax.nn.relu.html#jax.nn.relu) | [Relu](https://onnx.ai/onnx/operators/onnx__Relu.html) | v0.1.0 | [`relu`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/relu_model.onnx) ✅ |
| [flax.nnx.sigmoid](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.sigmoid) | [Sigmoid](https://onnx.ai/onnx/operators/onnx__Sigmoid.html) | v0.1.0 | [`sigmoid`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/sigmoid_model.onnx) ✅ |
| [flax.nnx.softmax](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.softmax) | [Softmax](https://onnx.ai/onnx/operators/onnx__Softmax.html) | v0.1.0 | [`softmax`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/softmax_model.onnx) ✅ |
| [flax.nnx.softplus](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.softplus) | [Softplus](https://onnx.ai/onnx/operators/onnx__Softplus.html) | v0.1.0 | [`softplus`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/softplus_model.onnx) ✅ |
| [flax.nnx.tanh](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.tanh) | [Tanh](https://onnx.ai/onnx/operators/onnx__Tanh.html) | v0.1.0 | [`tanh`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/tanh_model.onnx) ✅ |
| [jax.lax.slice](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.slice.html) | [Slice](https://onnx.ai/onnx/operators/onnx__Slice.html) | v0.1.0 | [`slice_basic`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/slice_basic_model.onnx) ✅<br>[`slice_last_column`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/slice_last_column_model.onnx) ✅<br>[`slice_out_of_bounds`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/slice_out_of_bounds_model.onnx) ✅<br>[`slice_single_element`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/slice_single_element_model.onnx) ✅<br>[`slice_with_stride`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/slice_with_stride_model.onnx) ✅ |
| [jax.numpy.add](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.add.html) | [Add](https://onnx.ai/onnx/operators/onnx__Add.html) | v0.1.0 | [`add`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/add_model.onnx) ✅ |
| [jax.numpy.concat](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.concat.html) | [Concat](https://onnx.ai/onnx/operators/onnx__Concat.html) | v0.1.0 | [`concat`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/concat_model.onnx) ✅ |
| [jax.numpy.einsum](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html) | [Einsum](https://onnx.ai/onnx/operators/onnx__Einsum.html) | v0.1.0 | [`einsum_attention`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/einsum_attention_model.onnx) ✅<br>[`einsum_batch_matmul`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/einsum_batch_matmul_model.onnx) ✅<br>[`einsum_matmul`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/einsum_matmul_model.onnx) ✅ |
| [jax.numpy.matmul](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html) | [MatMul](https://onnx.ai/onnx/operators/onnx__MatMul.html) | v0.1.0 | [`matmul_2d`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/matmul_2d_model.onnx) ✅<br>[`matmul_3d`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/matmul_3d_model.onnx) ✅<br>[`matmul_4d`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/matmul_4d_model.onnx) ✅ |
| [jax.numpy.reshape](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.reshape.html) | [Reshape](https://onnx.ai/onnx/operators/onnx__Reshape.html) | v0.1.0 | [`reshape`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/reshape_model.onnx) ✅<br>[`reshape_batch`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/reshape_batch_model.onnx) ✅<br>[`reshape_dynamic`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/reshape_dynamic_model.onnx) ✅ |
| [jax.numpy.squeeze](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.squeeze.html) | [Squeeze](https://onnx.ai/onnx/operators/onnx__Squeeze.html) | v0.1.0 | [`squeeze_multiple_dims`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/squeeze_multiple_dims_model.onnx) ✅<br>[`squeeze_no_change`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/squeeze_no_change_model.onnx) ✅<br>[`squeeze_single_dim`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/squeeze_single_dim_model.onnx) ✅<br>[`squeeze_vit_output`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/squeeze_vit_output_model.onnx) ✅ |
| [jax.numpy.tile](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html) | [Tile](https://onnx.ai/onnx/operators/onnx__Tile.html) | v0.1.0 | [`tile_1d`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/tile_1d_model.onnx) ✅<br>[`tile_2x`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/tile_2x_model.onnx) ✅<br>[`tile_batch_dim`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/tile_batch_dim_model.onnx) ✅<br>[`tile_large`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/tile_large_model.onnx) ✅ |
| [jax.numpy.transpose](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html) | [Transpose](https://onnx.ai/onnx/operators/onnx__Transpose.html) | v0.1.0 | [`transpose_4d`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/transpose_4d_model.onnx) ✅<br>[`transpose_basic`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/transpose_basic_model.onnx) ✅<br>[`transpose_high_dim`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/transpose_high_dim_model.onnx) ✅<br>[`transpose_reverse`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/transpose_reverse_model.onnx) ✅<br>[`transpose_square_matrix`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/transpose_square_matrix_model.onnx) ✅ |

<!-- AUTOGENERATED TABLE END -->

✅ = passed<br>
❌ = failed<br>
➖ = no tests, yet

Examples

<!-- AUTOGENERATED EXAMPLES TABLE START -->

| Component | Description | Children | Since | Testcases |
|:----------|:------------|:---------|:------|:---------|
| CNN | A MNIST CNN model with convolutional and linear layers. | flax.nnx.Conv<br>flax.nnx.Linear<br>flax.nnx.relu<br>flax.nnx.avg_pool<br>flax.nnx.reshape<br>flax.nnx.log_softmax | v0.1.0 | [`mnist_cnn`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_cnn_model.onnx) ✅ |
| CNN | A MNIST CNN model with convolutional, layer norm, pooling, dropout, and linear layers. | flax.nnx.Conv<br>flax.nnx.Linear<br>flax.nnx.relu<br>flax.nnx.avg_pool<br>flax.nnx.reshape<br>flax.nnx.log_softmax<br>flax.nnx.LayerNorm<br>flax.nnx.Dropout<br>flax.nnx.max_pool | v0.1.0 | [`mnist_cnn_2`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_cnn_2_model.onnx) ✅ |
| MLP | A simple Multi-Layer Perceptron (MLP) with BatchNorm, Dropout, and GELU activation. | flax.nnx.Linear<br>flax.nnx.BatchNorm<br>flax.nnx.Dropout<br>flax.nnx.gelu<br>flax.nnx.Linear | v0.1.0 | [`mlp`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mlp_model.onnx) ✅ |
| MLP Block | MLP in Transformer | flax.nnx.Linear<br>flax.nnx.Dropout<br>flax.nnx.gelu | v0.1.0 | [`mlp_block`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mlp_block_model.onnx) ✅ |
| MNISTConvolutionalTokenEmbedding | Convolutional Token Embedding for MNIST with hierarchical downsampling. | flax.nnx.Conv<br>flax.nnx.LayerNorm<br>jax.numpy.Reshape<br>jax.nn.relu | v0.1.0 | [`mnist_conv_embedding`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_conv_embedding_model.onnx) ✅ |
| PatchEmbedding | Cutting the image into patches and linearly embedding them. | flax.nnx.Linear<br>jax.numpy.Transpose<br>jax.numpy.Reshape | v0.1.0 | [`patch_embedding`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/patch_embedding_model.onnx) ✅ |
| TransformerBlock | Transformer from 'Attention Is All You Need.' | flax.nnx.MultiHeadAttention<br>flax.nnx.LayerNorm<br>MLPBlock<br>flax.nnx.Dropout | v0.1.0 | [`transformer_block`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/transformer_block_model.onnx) ✅ |
| ViT | A MNIST Vision Transformer (ViT) model | PatchEmbedding<br>TransformerBlock<br>flax.nnx.Linear<br>flax.nnx.LayerNorm | v0.1.0 | [`mnist_vit`](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_vit_model.onnx) ✅ |

<!-- AUTOGENERATED EXAMPLES TABLE END -->


Versions of Major Dependencies

| Library       | jax2onnx v0.1.0 | 
|:--------------|:----------------| 
| `JAX`         | v0.5.0          | 
| `Flax`        | v0.10.3         | 
| `onnx`        | v1.17.0         |  
| `onnxruntime` | v1.20.1         |  

Note: for more details look into the `pyproject.toml` file



### **Usage**
Import the `jax2onnx` module, implement the `to_onnx` function to your Module class and use the `jax2onnx.to_onnx.to_onnx`
function to convert your model to ONNX format. See at the examples provided in the `examples` directory.


Example of an MLP with the `to_onnx` function implemented:

```py
class MLP(nnx.Module):
    def __init__(self, din, dmid, dout, *, rngs=nnx.Rngs(0)):
        self.layers: list[Supports2Onnx] = [
            nnx.Linear(din, dmid, rngs=rngs),
            nnx.BatchNorm(dmid, rngs=rngs),
            nnx.Dropout(rate=0.1, rngs=rngs),
            PartialWithOnnx(nnx.gelu, approximate=False),
            nnx.Linear(dmid, dout, rngs=rngs),
        ]
    def __call__(self, x, deterministic = True):
        for layer in self.layers:
            if isinstance(layer, nnx.Dropout):
                x = layer(x, deterministic=deterministic)
            else:
                x = layer(x)
        return x
    def to_onnx(self, z, **params):
        for layer in self.layers:
            z = layer.to_onnx(z, **params)
        return z
```

To export the model to ONNX format, use the `to_onnx` function:
```py
from jax2onnx.to_onnx import to_onnx
to_onnx(
    model_file_name="mlp.onnx",
    component=MLP(din=30, dmid=20, dout=10, rngs=nnx.Rngs(17)),
    input_shapes=[(1, 30)]
)
```

### **How to Contribute**

If you'd like to see a model or function supported, consider contributing by adding a plugin for an existing   
module or function under the `jax2onnx/plugins` directory. Or you can add an example to the `examples` directory.
Certainly any other improvements are welcome as well.

### **Installation**

To install `jax2onnx`:

```bash
pip install -i https://test.pypi.org/simple/ jax2onnx
```

t.b.d.:
```bash
pip install jax2onnx  
```

### **License**
This project is licensed under the terms of the Apache License, Version 2.0. See the `LICENSE` file for details.

