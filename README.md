# jax2onnx


![img.png](https://enpasos.github.io/jax2onnx/images/img1.png)

`jax2onnx` converts your JAX/Flax model directly into the ONNX format.  

### **Approach**
Components can be easily added as plugins, including their test cases, which are automatically picked up by **pytest**. Each test case sends random input tensors through the JAX/Flax model and compares the output with the ONNX model to ensure correctness.

This library follows a **test-driven and demand-driven** approach, giving you **full control** over how JAX/Flax components are mapped to ONNX‚Äî**no hidden magic, no black-box abstraction**. While it may not cover every use case out of the box, you can **extend it by adding your own plugins** and contribute them back to the project. üöÄ

### **Supported JAX/ONNX Components**


 
<!-- AUTOGENERATED TABLE START -->

| JAX Component | ONNX Components | Testcases | Since |
|:-------------|:---------------|:---------|:------|
| [flax.nnx.BatchNorm](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.BatchNorm) | [BatchNormalization](https://onnx.ai/onnx/operators/onnx__BatchNormalization.html) | `batchnorm`  | v0.1.0 |
| [flax.nnx.Conv](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Conv) | [Conv](https://onnx.ai/onnx/operators/onnx__Conv.html) | `conv_3x3_1` <br>`conv_3x3_2` <br>`conv_3x3_3`  | v0.1.0 |
| [flax.nnx.Dropout](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/stochastic.html#flax.nnx.Dropout) | [Dropout](https://onnx.ai/onnx/operators/onnx__Dropout.html) | `dropout` <br>`dropout_1d` <br>`dropout_2d` <br>`dropout_3d` <br>`dropout_4d` <br>`dropout_high` <br>`dropout_low`  | v0.1.0 |
| [flax.nnx.LayerNorm](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.LayerNorm) | [LayerNormalization](https://onnx.ai/onnx/operators/onnx__LayerNormalization.html) | `layernorm_default` <br>`layernorm_multiaxis`  | v0.1.0 |
| [flax.nnx.Linear](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Linear) | [Gemm](https://onnx.ai/onnx/operators/onnx__Gemm.html)<br>[MatMul](https://onnx.ai/onnx/operators/onnx__MatMul.html) | `linear` <br>`linear_2d`  | v0.1.0 |
| [flax.nnx.LinearGeneral](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.LinearGeneral) | [Gemm](https://onnx.ai/onnx/operators/onnx__Gemm.html)<br>[MatMul](https://onnx.ai/onnx/operators/onnx__MatMul.html) | `linear_general` <br>`linear_general_2` <br>`linear_general_mha_projection` <br>`linear_general_mha_projection2`  | v0.1.0 |
| [flax.nnx.MultiHeadAttention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.MultiHeadAttention) | [Reshape](https://onnx.ai/onnx/operators/onnx__Reshape.html)<br>[Gemm](https://onnx.ai/onnx/operators/onnx__Gemm.html)<br>[Einsum](https://onnx.ai/onnx/operators/onnx__Einsum.html)<br>[Mul](https://onnx.ai/onnx/operators/onnx__Mul.html)<br>[Softmax](https://onnx.ai/onnx/operators/onnx__Softmax.html) | `multihead_attention`  | v0.1.0 |
| [flax.nnx.avg_pool](https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.avg_pool) | [AveragePool](https://onnx.ai/onnx/operators/onnx__AveragePool.html) | `avg_pool`  | v0.1.0 |
| [flax.nnx.dot_product_attention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.dot_product_attention) | [Constant](https://onnx.ai/onnx/operators/onnx__Constant.html)<br>[Einsum](https://onnx.ai/onnx/operators/onnx__Einsum.html)<br>[Mul](https://onnx.ai/onnx/operators/onnx__Mul.html)<br>[Softmax](https://onnx.ai/onnx/operators/onnx__Softmax.html) | `dot_product_attention` <br>`dot_product_attention_shape_check` <br>`dot_product_attention_softmax_axis`  | v0.1.0 |
| [flax.nnx.elu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.elu) | [Elu](https://onnx.ai/onnx/operators/onnx__Elu.html) | `elu`  | v0.1.0 |
| [flax.nnx.gelu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.gelu) | [Gelu](https://onnx.ai/onnx/operators/onnx__Gelu.html) | `gelu` <br>`gelu2` <br>`gelu3`  | v0.1.0 |
| [flax.nnx.leaky_relu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.leaky_relu) | [LeakyRelu](https://onnx.ai/onnx/operators/onnx__LeakyRelu.html) | `leaky_relu`  | v0.1.0 |
| [flax.nnx.log_softmax](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.log_softmax) | [LogSoftmax](https://onnx.ai/onnx/operators/onnx__LogSoftmax.html) | `log_softmax`  | v0.1.0 |
| [flax.nnx.max_pool](https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.max_pool) | [MaxPool](https://onnx.ai/onnx/operators/onnx__MaxPool.html) | `max_pool`  | v0.1.0 |
| [flax.nnx.relu](https://docs.jax.dev/en/latest/_autosummary/jax.nn.relu.html#jax.nn.relu) | [Relu](https://onnx.ai/onnx/operators/onnx__Relu.html) | `relu`  | v0.1.0 |
| [flax.nnx.sigmoid](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.sigmoid) | [Sigmoid](https://onnx.ai/onnx/operators/onnx__Sigmoid.html) | `sigmoid`  | v0.1.0 |
| [flax.nnx.softmax](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.softmax) | [Softmax](https://onnx.ai/onnx/operators/onnx__Softmax.html) | `softmax`  | v0.1.0 |
| [flax.nnx.softplus](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.softplus) | [Softplus](https://onnx.ai/onnx/operators/onnx__Softplus.html) | `softplus`  | v0.1.0 |
| [flax.nnx.tanh](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.tanh) | [Tanh](https://onnx.ai/onnx/operators/onnx__Tanh.html) | `tanh`  | v0.1.0 |
| [jax.lax.broadcast](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.broadcast.html) | [Expand](https://onnx.ai/onnx/operators/onnx__Expand.html) | `broadcast_a` <br>`broadcast_b`  | v0.1.0 |
| [jax.lax.gather](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.gather.html) | [Gather](https://onnx.ai/onnx/operators/onnx__Gather.html) | `gather_tf_out`  | v0.1.0 |
| [jax.lax.slice](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.slice.html) | [Slice](https://onnx.ai/onnx/operators/onnx__Slice.html) | `slice_basic` <br>`slice_last_column` <br>`slice_out_of_bounds` <br>`slice_single_element` <br>`slice_with_stride`  | v0.1.0 |
| [jax.numpy.add](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.add.html) | [Add](https://onnx.ai/onnx/operators/onnx__Add.html) | `add`  | v0.1.0 |
| [jax.numpy.concat](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.concat.html) | [Concat](https://onnx.ai/onnx/operators/onnx__Concat.html) | `concat`  | v0.1.0 |
| [jax.numpy.einsum](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html) | [Einsum](https://onnx.ai/onnx/operators/onnx__Einsum.html) | `einsum_attention` <br>`einsum_batch_matmul` <br>`einsum_dynamic_batch_matmul` <br>`einsum_dynamic_batch_matmul_batched` <br>`einsum_matmul` <br>`einsum_matmul_static`  | v0.1.0 |
| [jax.numpy.matmul](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html) | [MatMul](https://onnx.ai/onnx/operators/onnx__MatMul.html) | `matmul_2d` <br>`matmul_3d` <br>`matmul_4d`  | v0.1.0 |
| [jax.numpy.reshape](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.reshape.html) | [Reshape](https://onnx.ai/onnx/operators/onnx__Reshape.html) | `reshapeA` <br>`reshapeB` <br>`reshapeC` <br>`reshapeD` <br>`reshapeE`  | v0.1.0 |
| [jax.numpy.squeeze](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.squeeze.html) | [Squeeze](https://onnx.ai/onnx/operators/onnx__Squeeze.html) | `squeeze_dynamic_batch` <br>`squeeze_multiple_dims` <br>`squeeze_single_dim` <br>`squeeze_vit_output`  | v0.1.0 |
| [jax.numpy.tile](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html) | [Tile](https://onnx.ai/onnx/operators/onnx__Tile.html) | `tile_a` <br>`tile_b` <br>`tile_c`  | v0.1.0 |
| [jax.numpy.transpose](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html) | [Transpose](https://onnx.ai/onnx/operators/onnx__Transpose.html) | `transpose_4d` <br>`transpose_basic` <br>`transpose_high_dim` <br>`transpose_reverse` <br>`transpose_square_matrix`  | v0.1.0 |

<!-- AUTOGENERATED TABLE END -->

‚úÖ = passed<br>
‚ùå = failed

Examples

<!-- AUTOGENERATED EXAMPLES TABLE START -->

| Component | Description | Children                                                                                                                                                                          | Testcases | Since |
|:----------|:------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------|:------|
| CNN | A MNIST CNN model with convolutional and linear layers. | flax.nnx.Conv<br>flax.nnx.Linear<br>flax.nnx.relu<br>flax.nnx.avg_pool<br>flax.nnx.reshape<br>flax.nnx.log_softmax                                                                | `mnist_cnn_1`  | v0.1.0 |
| CNN | A MNIST CNN model with convolutional, layer norm, pooling, dropout, and linear layers. | flax.nnx.Conv<br>flax.nnx.Linear<br>flax.nnx.relu<br>flax.nnx.avg_pool<br>flax.nnx.reshape<br>flax.nnx.log_softmax<br>flax.nnx.LayerNorm<br>flax.nnx.Dropout<br>flax.nnx.max_pool | `mnist_cnn_2`  | v0.1.0 |
| ConvEmbedding | Convolutional Token Embedding for MNIST with hierarchical downsampling. | flax.nnx.Conv<br>flax.nnx.LayerNorm<br>jax.numpy.Reshape<br>jax.nn.relu                                                                                                           | `mnist_conv_embedding`  | v0.1.0 |
| MLP | A simple Multi-Layer Perceptron (MLP) with BatchNorm, Dropout, and GELU activation. | flax.nnx.Linear<br>flax.nnx.BatchNorm<br>flax.nnx.Dropout<br>flax.nnx.gelu                                                                                                        | `mlp`  | v0.1.0 |
| MLP Block | MLP in Transformer | flax.nnx.Linear<br>flax.nnx.Dropout<br>flax.nnx.gelu                                                                                                                              | `mlp_block`  | v0.1.0 |
| PatchEmbedding | Cutting the image into patches and linearly embedding them. | flax.nnx.Linear<br>jax.numpy.Transpose<br>jax.numpy.Reshape                                                                                                                       | `patch_embedding`  | v0.1.0 |
| TransformerBlock | Transformer from 'Attention Is All You Need.' | flax.nnx.MultiHeadAttention<br>flax.nnx.LayerNorm<br>MLPBlock<br>flax.nnx.Dropout                                                                                                 | `transformer_block`  | v0.1.0 |
| ViT | A MNIST Vision Transformer (ViT) model | ConvEmbedding<br>PatchEmbedding<br>TransformerBlock<br>flax.nnx.Linear<br>flax.nnx.LayerNorm<br>jax.lax.gather                                                                    | `mnist_vit_conv` [‚úÖ](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_vit_conv_00.onnx "static batch dim") [‚úÖ](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_vit_conv_10.onnx "static batch dim + more shape info") [‚úÖ](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_vit_conv_01.onnx "dynamic batch dim") [‚úÖ](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_vit_conv_11.onnx "dynamic batch dim + more shape info")<br>`mnist_vit_patch` [‚úÖ](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_vit_patch_00.onnx "static batch dim") [‚úÖ](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_vit_patch_10.onnx "static batch dim + more shape info") [‚úÖ](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_vit_patch_01.onnx "dynamic batch dim") [‚úÖ](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/mnist_vit_patch_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |

<!-- AUTOGENERATED EXAMPLES TABLE END -->


Versions of Major Dependencies

| Library       | jax2onnx v0.1.0 | 
|:--------------|:----------------| 
| `JAX`         | v0.5.1          | 
| `Flax`        | v0.10.5         | 
| `onnx`        | v1.17.0         |  
| `onnxruntime` | v1.20.1         |  

Note: for more details look into the `pyproject.toml` file

### **Limitations**
The onnx graph is composed in memory and then saved to disk. This may lead to memory issues for large models.


### **Usage**
Import the `jax2onnx` module, implement the `to_onnx` function to your Module class and use the `jax2onnx.to_onnx.to_onnx`
function to convert your model to ONNX format. See at the examples provided in the `examples` directory.


Example of an MLP with the `to_onnx` function implemented:

```py
class MLP(nnx.Module):
    def __init__(self, din, dmid, dout, *, rngs=nnx.Rngs(0)):
        self.layers: list[Supports2Onnx] = [
            nnx.Linear(din, dmid, rngs=rngs),
            nnx.BatchNorm(dmid, rngs=rngs),
            nnx.Dropout(rate=0.1, rngs=rngs),
            PartialWithOnnx(nnx.gelu, approximate=False),
            nnx.Linear(dmid, dout, rngs=rngs),
        ]
    def __call__(self, x, deterministic = True):
        for layer in self.layers:
            if isinstance(layer, nnx.Dropout):
                x = layer(x, deterministic=deterministic)
            else:
                x = layer(x)
        return x
    def to_onnx(self, z, **params):
        for layer in self.layers:
            z = layer.to_onnx(z, **params)
        return z
```

To export the model to ONNX format, use the `to_onnx` function:
```py
from jax2onnx import to_onnx
to_onnx(
    model_file_name="mlp.onnx",
    component=MLP(din=30, dmid=20, dout=10, rngs=nnx.Rngs(17)),
    input_shapes=[(1, 30)]
)
```
### **Supported Configurations**
 
 
<img src="https://enpasos.github.io/jax2onnx/images/variants.png" alt="variants" width="400">

| `internal shape info` | `dynamic batch dim` | Behavior |
|-----------------------|---------------------|----------|
| **True**              | **False**           | Default setting: Internal shape information is included, batch size is fixed. |
| **False**             | **False**           | Like the default, but without internal shape annotations. |
| **False**             | **True**            | Shape information is only provided for input and output tensors, with batch dimensions set dynamically (`'B'`). |
| **True**              | **True**            | Shape information is included on all connections where possible, with batch dimensions set dynamically (`'B'`). However, in cases where the batch dimension is merged with other dimensions internally, shape annotations must be omitted. |






### **How to Contribute**

If you'd like to see a specific model or function supported, consider contributing by adding a plugin for an existing module or function under the `jax2onnx/plugins` directory. You can also contribute by adding an example to the `examples` directory.

Of course, any other improvements are welcome as well!

### **Installation**

To install `jax2onnx` ... **t.b.d. - meanwhile use latest development version**:

```bash
pip install jax2onnx  
```

or to install the latest development version:

```bash
pip install -i https://test.pypi.org/simple/ jax2onnx
```
 

### **Pre and Post Transpose**

One of the challenges when converting models from JAX/Flax to ONNX is handling differences in tensor dimension conventions. For example, many JAX/Flax components (such as convolution layers) work with inputs in **NHWC** format (Batch, Height, Width, Channels), whereas ONNX‚Äôs convolution operator expects inputs in **NCHW** format (Batch, Channels, Height, Width).

To bridge this gap, `jax2onnx` supports two optional parameters when calling `to_onnx` on generating the ONNX model:

- **`pre_transpose`**:  
  A list of tuples that specify how to rearrange the dimensions of the **input tensor** before the conversion. For example, passing `[(0, 3, 1, 2)]` will convert a JAX/Flax input in NHWC format to ONNX‚Äôs NCHW format.  
 
- **`post_transpose`**:  
  A list of tuples that specify how to rearrange the dimensions of the **output tensor** after the ONNX conversion. This parameter is useful if you need to convert the output back from ONNX‚Äôs format to the expected JAX/Flax format (or any other preferred layout). For example, using `[(0, 2, 3, 1)]` will convert an output in NCHW format back to NHWC.  
 
In the **Conv** plugin, for instance, the input shape is first converted from ONNX (NCHW) to JAX (NHWC) using the helper function `onnx_shape_to_jax_shape`.

 
### **Dirty Details**
Mapping components between JAX and ONNX can be inherently challenging due to fundamental differences in how they represent computations. In JAX, a function and its parameters are evaluated dynamically at runtime, whereas in ONNX, the computational graph is static and defined at conversion time (via JAX2ONNX) before being used at runtime.

Rather than attempting to match all possible runtime parameters of a JAX component to an ONNX equivalent, we focus on mapping only those relevant to a specific use case. This tradeoff between simplicity and generality ensures practicality but requires awareness of the intended use case. There are two points in time when the use case can be known:
1. **During Testing** ‚Äì In unit tests, we explicitly define the parameters passed to the `to_onnx` function, which is monkey-patched onto the component. To ensure the correct mapping of JAX function parameters to their ONNX counterparts, `to_onnx` must return a function that accepts the JAX function parameters. An example of this approach can be found in `jax2onnx/plugins/jax/lax/slice.py`.
2. **During Usage** ‚Äì Ideally, our `to_onnx` implementation allows users to use JAX components as they normally would. However, in some cases, users may need to wrap a component in one of our `PartialWithOnnx` or `Supports2Onnx` classes. These wrappers enable fine-tuning of `to_onnx` behavior in conjunction with the component‚Äôs JAX `__call__` function. For an example, see `ReshapeWithOnnx(Supports2Onnx)` in `tests/examples/mnist_cnn.py`.

This approach ensures flexibility while maintaining compatibility between JAX and ONNX, balancing ease of use with the necessary constraints of a static computational graph.



### **License**
This project is licensed under the terms of the Apache License, Version 2.0. See the `LICENSE` file for details.

  

