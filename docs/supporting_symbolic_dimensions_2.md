# Supporting Symbolic Dimensions in **jax2onnx** – Feasibility & Plan

## Feasibility of Symbolic Shapes in JAX 0.6.0

**JAX shape polymorphism** is now a supported feature in recent JAX versions (0.4+ and expected in 0.6.0) for tracing with unknown dimensions. JAX introduces **dimension variables** (e.g. `DimVar`) to represent unknown sizes, and these can appear in array shapes during tracing. The official JAX docs for JAX–TensorFlow conversion confirm that users can specify polymorphic shapes by providing dimension names (like `"b"`) in a shape specification. This causes JAX to treat that dimension as a symbolic variable during tracing ([jax/jax/experimental/jax2tf/README.md at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#:~:text=The%20,this%20particular%20example%2C%20we%20can)). Dimension variables are assumed to range over all positive integers (i.e. they stand for an *unknown size* ≥1). JAX’s shape-polymorphic tracing can even handle relationships between symbolic dims (e.g. requiring two dims to be equal by using the same symbol) ([jax/jax/experimental/jax2tf/README.md at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#:~:text=In%20the%20example%20above%2C%20the,for%20the%20unknown%20image%20size)) ([jax/jax/experimental/jax2tf/README.md at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#:~:text=The%20JAX%20tracing%20mechanism%20performs,jnp.matmul)). In short, JAX 0.6.0 is designed to allow JAX transformations (jit, make_jaxpr, etc.) to proceed with **abstract shapes containing symbols** instead of concrete sizes.

Under the hood, these symbolic dimensions are represented by JAX internal classes like `DimVar` (for a single named dimension) and `_DimExpr` (for expressions involving dimension variables). For example, if an output dimension is computed as twice an unknown input `B`, the resulting abstract shape might contain a `_DimExpr` representing `2 * B`. JAX’s tracing and abstract evaluation can propagate such symbolic expressions. Indeed, JAX developers note that a shape “may be _DimExpr, not just DimVar” in shape-polymorphic contexts ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=vjp_polymorphic_shapes%20%3D%20tuple)). Users can even specify simple expressions in the polymorphic shape strings (e.g. `"2 * height"` in place of `"height"`) to impose constraints or transformations on a symbolic dimension ([Shape Polymorphism, with an image downscale · jax-ml jax · Discussion #15995 · GitHub](https://github.com/google/jax/discussions/15995#:~:text=At%20some%20level%20this%20is,in%20the%20polymorphic%20shape%20specification)). This confirms that **`jax.export.symbolic_shape`** (or related APIs) are intended to parse both simple names and expressions into JAX’s symbolic dimension objects.

Crucially, JAX provides high-level APIs that consume these polymorphic shapes. The function `jax.eval_shape` (and by extension `jax.make_jaxpr`) fully supports shapes with symbolic dimensions. For example, using `jax.eval_shape` on a function with inputs specified via symbolic shapes returns a `ShapeDtypeStruct` whose `.shape` may include dimension symbols or expressions ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=,with)). This is exactly how JAX’s jax2tf conversion handles polymorphic shapes: it parses the user’s shape strings into dimension variables, supplies them in a `ShapeDtypeStruct` for each argument, and then calls `jax.eval_shape` or similar to get the output’s abstract shape ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=args_poly_specs%20%3D%20export)). The JAX code confirms this is expected – *“the shape may contain symbolic dimension expressions”* in the result of `jax.eval_shape` ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=,with)). In practice, JAX’s internal tracing (which underlies `make_jaxpr`, `jit`, etc.) will accept `ShapeDtypeStruct` inputs with symbolic dims and produce a JAXpr that carries those symbols. Therefore, **feeding `ShapeDtypeStruct` with `DimVar` dims into `jax.make_jaxpr` is safe** – it will yield an abstract JAXpr where shapes are symbolic. This is already leveraged by JAX’s export pipeline and tests to generate polymorphic JAXprs for shape-polymorphic functions ([jax/jax/experimental/jax2tf/README.md at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#:~:text=The%20JAX%20tracing%20mechanism%20performs,jnp.matmul)).

**Conclusion:** It is absolutely feasible to support symbolic dimensions in **jax2onnx**. The underlying JAX machinery (as of 0.6.0) is designed to handle unknown dimensions via `DimVar` and friends. Both JAX’s abstract eval rules and JAXpr tracing can operate with these symbols in place of concrete sizes. Indeed, JAX’s own conversion tools (jax2tf, SavedModel export) rely on this to allow “batch polymorphic” models where batch size is symbolic ([jax/jax/experimental/jax2tf/README.md at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#:~:text=%60tf.TensorSpec%28,function)) ([jax/jax/experimental/jax2tf/README.md at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#:~:text=The%20,for%20a%20series%20of)). Thus, jax2onnx can piggy-back on the same mechanisms to enable symbolic batch dimensions (or other dims) in its conversion process. No fundamental blocker exists in JAX – symbolic shapes are a first-class concept for abstract tracing.

## Approach: Converting User Strings to JAX Symbolic Dims

To maintain a simple user interface, jax2onnx will allow users to specify symbolic dimensions as **strings** (e.g. `"B"` for a batch dimension) in the `to_onnx()` input signature. Internally, we’ll transform those strings into JAX’s symbolic dimension objects *before* tracing the function. The plan is:

1. **Parse Input Shapes:** In the jax2onnx front-end (likely in `parameter_validation` or `user_interface`), detect any dimensions given as strings. For example, if the user provided an input shape `( "B", 224, 224, 3 )`, identify `"B"` as a symbolic dim name.

2. **Convert to DimVar:** Utilize `jax.export.symbolic_shape` (from the JAX library) to convert each dimension name or expression string into the corresponding JAX shape object. This function is part of JAX’s export utilities designed exactly for this purpose. In JAX 0.6.0’s implementation, it will return a `DimVar` instance (or `_DimExpr` for expressions) representing that dimension. Essentially, `jax.export.symbolic_shape("B")` will yield a fresh dimension variable labeled "B" that JAX tracing will treat as an unknown size. Likewise, a string like `"2*B"` or `"B+3"` would yield a `_DimExpr` combining that variable ([Shape Polymorphism, with an image downscale · jax-ml jax · Discussion #15995 · GitHub](https://github.com/google/jax/discussions/15995#:~:text=At%20some%20level%20this%20is,in%20the%20polymorphic%20shape%20specification)). (The JAX polymorphic shape parsing logic is used internally by `jax.export.symbolic_args_specs` – which splits a polymorphic shape string into per-dimension objects – so we are leveraging an intended use-case of the API.)

3. **Construct Abstract Inputs:** Replace the string entries in the shape with these symbolic objects. Then create JAX abstract values for each input, typically using `jax.ShapeDtypeStruct(shape, dtype)`. For example, after conversion, the shape `(DimVar(B), 224, 224, 3)` along with a dtype (say float32) would form a `ShapeDtypeStruct` placeholder. We will pass these to the tracer instead of concrete example arrays. This signals JAX to perform abstract evaluation. (JAX’s docs note that `jax.eval_shape` and conversion APIs accept `ShapeDtypeStruct` or any object with `.shape` and `.dtype` ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=Returns%3A%20a%20function%20that%20takes,s%20%28or%20any%20values)).)

4. **Trace to JAXpr:** Modify the `trace_jaxpr` routine (or equivalent function that obtains the JAXpr and output avals in jax2onnx) to use the symbolic shapes. If previously `trace_jaxpr` simply did something like `jax.make_jaxpr(fun)(*args)` with concrete dummy inputs, we will change it to use our prepared abstract inputs instead. Concretely, we can call `jax.eval_shape(fun, *abstract_inputs)` to get output aval(s) *and* use `jax.make_jaxpr` on the function by passing the same abstract inputs. Because the inputs carry symbolic dims, the resulting JAXpr’s abstract shapes will include `DimVar("B")` where appropriate, rather than fixed numbers. This directly leverages JAX’s shape-polymorphic tracing. (If using `jax.make_jaxpr`, we might need to wrap the inputs in a way that JAX treats them as abstract – one way is indeed using `ShapeDtypeStruct` as arguments. JAX will not try to treat those as real arrays but as already-abstract.)

5. **Propagate through Conversion:** The obtained JAXpr and output abstract values (avals) will feed into the ONNX conversion logic (`jaxpr_converter`). Because the avals contain symbolic shapes, our ONNX graph builder can assign those symbolic dimensions to ONNX dimensions. ONNX IR supports symbolic dimensions via the `dim_param` field in Tensor shapes. The converter should map each unique JAX `DimVar` to a unique ONNX `dim_param` (likely using the same name string if possible). For example, a JAX shape `(DimVar(B), 1000)` could become an ONNX TensorShape with `dim_param="B"` for the first dimension and dim_value=1000 for the second. We should ensure that the same symbol name is reused consistently (JAX uses object identity for DimVars – e.g., if two inputs share the same symbol, they refer to the same unknown size). JAX2onnx should reflect that in ONNX (ONNX will treat identical `dim_param` strings as the same symbolic dimension during shape inference). 

This approach is **directly aligned with JAX’s intended usage**. In fact, JAX provides a helper `export.symbolic_args_specs` that essentially does steps 1–3 for a whole argument list given a list of polymorphic shape strings ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=def%20do_eval_polymorphic_shape%28%2Aargs_specs%29%20)). (It returns a list of `ShapeDtypeStruct` with symbolic shapes inserted, which JAX2TF then passes to `jax.eval_shape`.) We can mirror this in jax2onnx: for each input, take the user’s shape specification; if a corresponding polymorphic shape string is provided, apply `jax.export.symbolic_shape` or the higher-level `symbolic_args_specs` to get the abstract arg. Then trace as usual. The **feasibility is confirmed by JAX2TF’s example**: given `polymorphic_shapes=["a, b", "b, c"]` for a function of two matrices, JAX produced an output spec with shape `(a, c)` – here `a, b, c` are `DimVar` symbols that flowed through matrix multiplication shape inference ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=,float32)). JAX2onnx can achieve the same effect for ONNX export.

## Integration with jax2onnx Codebase

Implementing this will involve updates in a few places:

- **Input Signature Handling:** Update `parameter_validation.py` (or wherever input shapes are validated and normalized) to accept `str` as a valid dimension spec. Currently, it likely only allows integers (and possibly `None`). We will allow strings and treat them as symbolic dim names. For example, if `None` was used to signify a dynamic dim before (as in test_dynamic_batch), we might deprecate that in favor of explicit symbols. Each distinct string (other than special placeholders like `"_" or "..."` if we choose to support those) will correspond to a distinct dimension variable in that tracing context.

- **trace_jaxpr / Conversion Routine:** In `conversion_api.py` or `user_interface.py`, where we construct the JAX abstract inputs and trace the function, insert a transformation of shapes. Concretely, we create the list of `jax.ShapeDtypeStruct` for function arguments. Pseudocode:
  ```python
  import jax
  from jax.experimental import export  # (assuming jax.export is available)
  # ... gather user-provided shapes and dtypes ...
  polymorphic_specs = [...]  # e.g. ["B, 224, 224, 3", ...] based on user strings or some structure
  arg_specs = [jax.ShapeDtypeStruct(shape_tuple, dtype) for each arg] 
  args_poly = export.symbolic_args_specs(arg_specs, polymorphic_specs)
  # Now args_poly contains ShapeDtypeStructs with DimVar in places of "B"
  out_shape = jax.eval_shape(fun, *args_poly)  # get output aval (optional)
  jaxpr = jax.make_jaxpr(fun)(*args_poly)      # get jaxpr with symbolic shapes
  ```
  This is essentially what JAX does internally for shape-polymorphic conversion ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=def%20do_eval_polymorphic_shape%28%2Aargs_specs%29%20)). If `jax.export.symbolic_args_specs` is not publicly accessible, we can manually map each shape element via `jax.export.symbolic_shape(dim_str)` as needed. The key is that **we call `jax.make_jaxpr` or `jax.eval_shape` outside of any tracing context**, so these symbolic dims are treated as *static abstract symbols* rather than tracers. This avoids the previous problem of accidental Tracers leaking into shape computations. By converting to `DimVar` up front, we ensure shape computations occur in Python with symbolic objects, not with JAX Tracer objects.

- **Abstract Evaluation of Custom Primitives:** jax2onnx’s plugin system allows custom JAX primitives (or wrappers) to be converted to ONNX. Each custom primitive typically has an `abstract_eval` (abstract interpretation rule) that computes output shapes from input shapes. With this new system, those input shapes may contain `DimVar` or `_DimExpr` rather than concrete ints. We should audit the existing custom primitive shape functions to ensure they handle symbolic values. **Fortunately, simple shape arithmetic works out-of-the-box** – `DimVar` supports basic Python operations like addition, multiplication, etc., producing `_DimExpr` results. This means if an abstract eval does something like `out_height = x_shape[0] // 2`, and `x_shape[0]` is a `DimVar('B')`, the `// 2` will produce a `_DimExpr(floor_div(B, 2))` object. As long as the abstract eval returns a `ShapedArray` or `ShapeDtypeStruct` with that `_DimExpr` in its shape, JAX will accept it. **Example:** In JAX’s own code, if you have `a.shape = (a, c)` with `a` a DimVar, then an operation like `x.T` yields shape `(c, a)` (swapping the symbols), which is represented as tuple `(DimVar('c'), DimVar('a'))` – JAX handles this and knows that, e.g., if originally `a == c` due to a constraint, that relation persists ([jax/jax/experimental/jax2tf/README.md at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#:~:text=The%20JAX%20tracing%20mechanism%20performs,jnp.matmul)). So, custom shape logic that uses standard Python arithmetic and indexing on shapes should naturally work with symbolic dims.

  However, any plugin abstract_eval that tries to **use shapes in a raw Python context that requires concreteness** must be adjusted. For instance, calling `int(dim)` or using a dimension in a comparison that cannot be resolved symbolically will raise an error. Plugin authors should avoid forcing symbolic dims to concrete values. Instead, they can propagate the symbols. If truly needed, one could embed shape logic in a JAX function and use `jax.eval_shape` inside abstract_eval (now possible since we’re outside tracing) – but this is likely overkill. In most cases, treating the symbolic dimension as an opaque int-like object and performing algebraic ops is sufficient.

- **Mapping Symbols to ONNX:** In the ONNX graph construction (likely in `onnx_builder.py` or the lower parts of `jaxpr_converter.py`), ensure that whenever a JAX shape is converted to an ONNX shape, the presence of a `DimVar` or `_DimExpr` is handled. The converter should attach a name for that dimension in the ONNX `ValueInfo`. Typically, ONNX uses `dim_param` for symbolic dimensions. We can use the string name of the `DimVar` (JAX’s `DimVar` likely has a `.name` or `__str__` that gives the original “B”). Indeed, in JAX2TF they simply do `str(dim)` to get a string like `"a"` or `"2*a"` ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=vjp_polymorphic_shapes%20%3D%20tuple)), which they then pass as the polymorphic_shapes argument for subsequent stages. We can do similarly: use `str(symbolic_dim)` as the ONNX `dim_param`. For pure `_DimExpr` (like `2*B`), `str()` will yield a string expression. ONNX cannot understand arithmetic in `dim_param` (it’s just a name), so **caveat**: if our shapes produce expressions, we might need a strategy. One approach is to assign a fresh symbol name for that expression (since ONNX shape inference won’t evaluate `2*B`). For example, if an output dimension is `2*B`, we could name it something like `B_times_2` or simply treat it as another symbol (the user might ultimately need to handle that logic). This is an edge case: many ONNX ops don’t preserve complex relations between symbolic dims, except by encoding them as separate symbols. So it’s acceptable to represent `2*B` as a distinct symbolic dim (ONNX will not know it equals 2x another dim). Documenting this limitation would be wise. In summary, each JAX `DimVar` yields a unique ONNX symbol name (often the same letter), and each distinct `_DimExpr` could either be left as a string (which ONNX would treat as just another symbol name) or simplified if possible. The **risk** is minimal – ONNX shape inference will treat different symbol names as independent unknowns, which is usually fine.

## Benefits for Custom Primitive Shape Inference

By having symbolic dimensions flow through the JAX tracing, we unlock more powerful shape inference for custom ops in jax2onnx:

- **Abstract Eval with Symbols:** Custom primitives can now leverage the actual *symbolic values* during shape computation. For instance, if a custom primitive’s output shape is `(input_dim + 1) x  C`, and the input’s first dimension is symbolic `B`, the primitive’s abstract eval can produce shape `(B + 1, C)` as a `_DimExpr`. Downstream, the ONNX converter will see that and can propagate it accordingly. Without symbolic support, jax2onnx might have forced users to pick a concrete example size or simply treat everything as unknown `None` in ONNX. This change means shape relationships are preserved.

- **Using `jax.eval_shape` Safely:** If a custom primitive’s abstract eval is complex, one strategy is to implement a dummy shape-calculation function using JAX numpy (which can include conditionals or complex math) and call `jax.eval_shape` on it inside the abstract eval. Previously, doing this could fail because if any input was a JAX Tracer (due to being in the middle of tracing), `jax.eval_shape` would complain (it expects static shapes, not tracers). Now, however, our inputs to abstract eval are not tracers but `ShapedArray` with possible `DimVar` entries. This means inside `abstract_eval` we are in Python context with symbolic dims, so calling `jax.eval_shape` on a sub-function will work – the sub-function will see a `DimVar` as an integer-like value (likely as a special polymorphic dimension). In essence, **no actual tracing** happens when you call `jax.eval_shape` with a `ShapeDtypeStruct`; it directly returns an abstract result ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=args_poly_specs%20%3D%20export)). This enables advanced shape logic without hacks. We should verify this on JAX 0.6.0 source, but given that jax2tf uses `jax.eval_shape` on symbolic args (as shown by `jax.eval_shape(fun_jax, *args_poly_specs)` in the code ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=args_poly_specs%20%3D%20export))), it is clearly a supported use pattern. Thus, plugin authors can confidently use `jax.eval_shape` inside their `abstract_eval` rules if needed, knowing the inputs are static symbolic shapes (not dynamic tracers). This addresses prior issues where shape functions would error out due to tracer leakage.

## Risks and Caveats

While the plan is solid, a few considerations and potential pitfalls:

- **Ensure Unique Symbol Context:** JAX’s dimension variables are function-scoped. If the user reuses the same string in multiple inputs intending them to be the *same* dimension, JAX’s parser will indeed treat them as one shared `DimVar`. (For example, `polymorphic_shapes=["B, C", "B, D"]` would mean two inputs share the first-dimension symbol `B`.) We must preserve this behavior. In jax2onnx’s interface, if a user uses the same string `"B"` in two input shapes, we should pass a single symbol object for both, not two separate ones. The `jax.export.symbolic_args_specs` utility likely handles this automatically when given a list of shape strings for all args ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=,float32)). If we implement manual conversion, we should reuse the `DimVar` when names repeat. Perhaps maintain a dict mapping dim name to JAX symbol during parsing.

- **Backward Compatibility:** If previously `None` or `-1` was used in jax2onnx to denote a dynamic dimension, we should still accept those (for backward compat) but convert them internally to a new symbol (e.g. treat each `None` as a unique symbol or allow user to specify a name). Encouraging explicit naming (“B”) is better for clarity. Tests like `test_dynamic_batch.py` might currently expect a certain behavior (e.g. passing `None` yields an ONNX model with an unspecified dim). After our change, `None` could be mapped to a default symbol name like `"N0"` etc.

- **ONNX Inference Limitations:** ONNX’s shape inference engine does not perform arithmetic on symbolic dims. So even though JAX knows, say, output dim = 2*B, ONNX will just see a symbol. This is generally fine – the ONNX model remains polymorphic, but shape relationships might not be enforced or obvious in ONNX. In most cases (like batch size propagation), this isn’t a big issue. It’s just something to note: the rich relationships in JAX (like equality of two dims) won’t automatically become ONNX constraints beyond reusing the same `dim_param`. For example, if output has shape `(B, B)` and we set both dims’ `dim_param="B"`, ONNX does understand they’re the same unknown. But if output is `(B, B+1)`, we might label first dim as `"B"` and second as, say, `"B_plus_1"` – ONNX won’t know the second is exactly one more. This is an **acceptable trade-off**, as ONNX cannot encode that dependency anyway. The key point is that allowing symbolic dims at all is a net gain (before, one might have had to hardcode one size).

- **Testing & Community Experience:** We should test this feature with known examples. For instance, the jax2onnx `test_symbolic_dim_name.py` likely covers passing a symbolic string and expecting the ONNX model to have that `dim_param`. Also, any custom primitive shape computations in the plugins should be tested with symbolic inputs. Community discussions (like the StackOverflow question on JAX dynamic shapes) indicate that earlier JAX didn’t support dynamic shapes, but now shape polymorphism fills that gap. The JAX team has marked shape-polymorphic conversion as experimental but actively improved it ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=during%20shape%20evaluation,jax2tf.convert)). Given jax2tf and SavedModel export already use it, the risk is low. If any edge case arises (e.g. a specific primitive doesn’t support abstract evaluation with symbols), it would typically manifest as a JAX error during tracing – we can document and possibly catch such cases (maybe giving a clear error if a particular operation requires static shapes).

- **Performance Impact:** Using symbolic dims should not significantly affect tracing performance. JAX will still compile a single XLA computation with symbolic assumptions (in fact, it will compile as if those dims are some abstract size, often treating them like dynamic shapes internally). The ONNX conversion time might increase slightly if we need to manipulate string names and ensure consistency, but that’s negligible. The generated ONNX model will be slightly more general (which is a positive). We just need to ensure that when serializing to ONNX, we don’t accidentally hardcode a dummy value for `B`. The model’s GraphProto should leave it symbolic (with `dim_param`).

In summary, **supporting symbolic dimensions in jax2onnx is not only feasible but aligns with the direction of JAX’s design**. By converting user-provided dimension names to JAX’s `DimVar` objects via `jax.export.symbolic_shape`, we allow JAX’s tracing to natively handle unknown sizes. The abstract evaluation of shapes (even for custom ops) will naturally utilize these symbolic dims – meaning shape formulas rather than fixed numbers. JAX’s own code and documentation confirm this is a supported use: *“Dimension variables… stand for shape dimensions that are unknown at tracing time”* ([jax/jax/experimental/jax2tf/README.md at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#:~:text=The%20,this%20particular%20example%2C%20we%20can)) and JAX will carry them through shape calculations. The outcome will be an ONNX model that is polymorphic in those dimensions (useful for allowing, say, any batch size `"B"` at runtime). This solution ties into jax2onnx at the points where we generate the JAXpr and where we construct ONNX shape info. With careful implementation, the user experience remains simple (just use `"B"` in your shape tuple), while internally we harness JAX’s robust shape-polymorphic tracing to do the heavy lifting ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=,with)) ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=,float32)).

**References:**

- JAX2TF documentation on shape polymorphic conversion ([jax/jax/experimental/jax2tf/README.md at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#:~:text=The%20,specification%20you%20can%20use)), confirming the concept of dimension variables (e.g. `"b"`) for unknown dimensions.
- JAX2TF code example using `jax.eval_shape` with symbolic shapes, showing the result’s shape contains symbols ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=,with)).
- Discussion by JAX devs (Google) on using expressions like `2 * height` in polymorphic shape specs ([Shape Polymorphism, with an image downscale · jax-ml jax · Discussion #15995 · GitHub](https://github.com/google/jax/discussions/15995#:~:text=At%20some%20level%20this%20is,in%20the%20polymorphic%20shape%20specification)), illustrating how `DimVar` and `_DimExpr` handle simple arithmetic.
- JAX internal code note that shapes may be `_DimExpr` (composed symbols), not just raw ints ([jax/jax/experimental/jax2tf/jax2tf.py at main · jax-ml/jax · GitHub](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#:~:text=vjp_polymorphic_shapes%20%3D%20tuple)) – indicating JAX’s ability to represent computed dimensions symbolically.