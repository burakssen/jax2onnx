# Handling Inputs in ONNX If Operator Subgraphs

## 1. Real-World ONNX Models Using the `If` Operator

In practice, ONNX models that include the `If` operator do **not** explicitly list parent-scope tensors as subgraph inputs. Both the `then_branch` and `else_branch` are defined with **empty input lists**, yet they freely use values from the outer graph by name. For example, a PyTorch-exported model with a conditional has subgraphs that capture outer tensors implicitly. In one such model, the `else_branch` is defined with `inputs=()` but its nodes reference an outer tensor `"linear_1"` directly (see the `Neg` node input below). This means the subgraph “captures” `linear_1` from the parent scope without declaring it as a formal input. All needed variables (like `linear_1` in this example) are assumed to be present in the enclosing graph and are accessed by name inside the branches.

Public ONNX models converted from frameworks follow this pattern. For instance, TensorFlow 2 models that use `tf.cond` (such as some object detection models like EfficientDet or CenterNet) end up with ONNX `If` nodes that capture required tensors implicitly. If you attempt to add extra inputs to the `If` node itself, the model will be invalid. One user who tried to feed an additional tensor into an `If` found that ONNX threw an error: *“input size of if-op should be 1”*. In other words, the ONNX `If` operator only accepts the single boolean condition as an input – any other needed data must **not** be passed as a second input. Instead, as real-world examples show, those values are simply referenced by name inside the subgraphs.

## 2. ONNX Runtime Behavior and Subgraph Variable Resolution

**ONNX Runtime and the ONNX spec resolve subgraph references by implicit capture.** The subgraphs attached to an `If` node (as well as other control-flow ops like `Loop`) are allowed to refer to outer-scope variables directly. According to ONNX maintainers, *“the then and else branches have no inputs by design. The subgraphs used as attributes of control-flow ops are allowed to refer to variables defined in the outer scope. Hence, there is no need to pass in input parameters to the then or else branch”*. This design treats the subgraph like a closure that inherits its parent context, much as an `if` statement in a programming language can use variables defined outside the `if`. In practical terms, any tensor name used inside a branch is looked up in the parent graph’s namespace at runtime (or during graph linking), without being listed as an explicit parameter of the subgraph.

All variables that a branch needs **must therefore exist in the parent graph** (either as an input, initializer, or an earlier node’s output). If a subgraph incorrectly declares its own inputs or the needed value is not present, the model will fail validation or inference. For example, if you construct an `If` node where a branch GraphProto has an input that wasn’t wired through, ONNX’s checker or shape-inference will error out with a message such as: *“Size mismatch validating subgraph inputs. Got 0 inputs but subgraph has 1… Either provide all subgraph inputs, or just the required inputs.”*. In essence, **ONNX If expects zero explicit subgraph inputs** (beyond the condition). ONNX Runtime will automatically make any outer-scope tensor with a matching name available inside the branches. This implicit capture mechanism is the only way branch subgraphs receive data from the enclosing graph – you cannot manually “feed” tensors into an If node aside from the boolean condition.

## 3. How Exporters Handle `If` Subgraphs (PyTorch, TensorFlow, etc.)

Major ONNX exporters all adhere to the above approach, injecting needed values by name rather than as formal inputs to the `If` node:

* **PyTorch (torch.onnx):** The PyTorch ONNX exporter emits `If` nodes with subgraphs that have no declared inputs. Any tensor used within a branch will be captured from the main graph. A user observing a TorchScript-exported model noted that the `If` node’s then/else subgraphs were “without inputs” – some branch nodes appeared disconnected in graph visualization, yet they correspond to outer graph values. This is intentional: the exporter ensures that a value needed in a branch (e.g. the result of a prior layer) is computed in the outer graph (before the If), and the subgraph simply refers to that value by name. PyTorch’s exporter thus does **not** create extra parameters on the If; it relies on ONNX’s implicit capture semantics to provide branch variables.

* **TensorFlow (tf2onnx):** The tf2onnx converter similarly does *not* add extra inputs to the `If`. When converting `tf.cond` or other conditionals, it generates an ONNX `If` with only the condition input. Any tensor that both branches need is referenced in those subgraphs by name. ONNX contributors have explained that the `then_branch` and `else_branch` GraphProtos “refer to outside inputs” and there is *“no need to pass those values to the If node.”* In other words, tf2onnx will create an If node where the branches implicitly capture required tensors (for example, a tensor produced by a previous node like a ReLU in the main graph). If an exporter or user mistakenly tries to treat those as formal inputs, it breaks the ONNX model. In fact, OpenVINO’s IR converter encountered an error in one such case, stating *“Number of inputs to 'then\_branch' is invalid. Expected 0, actual 1”*, because the branch was expecting an outer tensor named `"encoder_16"` as an input. The correct approach (and what tf2onnx does) is to leave the branch input list empty so that `"encoder_16"` is implicitly captured from the parent graph. As Justin Chu (ONNX contributor) emphasized, *“then\_branch should not have an input. You may assume existence of the value in the subgraph.”* This is exactly how tf2onnx implements conditional subgraphs.

* **Other Converters (OpenVINO, WinMLTools, etc.):** Tools that consume or produce ONNX generally expect this same convention. OpenVINO’s Model Optimizer (which reads ONNX models) assumes that `If` subgraphs have no inputs. If a model is constructed otherwise, it will flag an error as seen above. In practice, most high-level converters (Keras/WinMLTools, MXNet, etc.) rarely need to generate an `If` node since many frameworks do not use dynamic control flow in exported graphs. But if they do, they must conform to the ONNX spec: only one explicit input (the condition) and all branch dependencies resolved by name capture. We found no evidence of any exporter using an alternate scheme (such as manually wiring additional inputs into the If node). The de facto standard across frameworks is to rely on ONNX’s implicit outer-scope resolution for subgraph variables.

## Best Practices and De Facto Standard for `If` Subgraphs

**The consensus best practice is to treat `If` subgraphs as closures that implicitly capture needed tensors from the parent graph.** In ONNX, an `If` node should be constructed with exactly one input (the boolean condition) and *N* outputs (matching the branches’ outputs). Any tensor needed inside the then/else graphs (other than constants created within the subgraph) should be produced in the outer graph prior to the If. The branch GraphProtos list **no inputs** – they simply use the parent-scope values by name. All major exporters follow this pattern, and ONNX Runtime’s execution model is built around it.

In summary, you do **not** explicitly pass parent tensors into an If node’s branches. Instead, ensure the tensor is available in the outer graph and refer to it directly in the subgraph. This design was chosen to mirror typical programming semantics (an if-statement can use variables in the outer scope). While this can make the graph appear to have “dangling” inputs to a casual viewer, it is the intended behavior. Tools that manipulate ONNX graphs need to handle these control-flow subgraphs by recognizing that their true inputs come from the enclosing scope, even if not listed. By following the implicit-capture convention, ONNX models with `If` operators will be consistent and compatible with ONNX Runtime and other frameworks’ expectations. All evidence indicates this approach is the de facto standard for ONNX conditional subgraphs, and deviating from it (e.g. by trying to enumerate branch inputs explicitly) will lead to errors or incompatibilities. The safe rule is: **pass only the condition to the If node, and rely on the ONNX runtime to provide any other needed tensors to the branch code via name matching.**
